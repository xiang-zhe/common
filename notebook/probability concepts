
1，熵
  1.1， 离散变量 i 的概率分布P(i)，熵的公式： entropy = -ΣP(i)logP(i)
  1.2， 真实概率分布P和假设概率分布Q，则交叉熵: H(P,Q) = -ΣP(i)logQ(i)
      二分类交叉熵：BinaryCrossEntropy = -PlogQ-(1-P)log(1-Q)
2，KL离散度（Kullback-Leibler Divergence ）
  衡量两个分布的差异程度，KL值越小，差异越小；
  两个概率分布P和Q，KL离散度：D-KL(P||Q) = ΣP(i)log(P(i)/Q(i))
  通常在概率和统计中，我们会用更简单的近似分布来代替观察到的数据或复杂的分布。KL散度帮助我们衡量在选择近似值时损失了多少信息。
  
3，贝叶斯公式
  P(A|B)P(B) = P(B|A)P(A)
  
4，先验与后验
  从原因到结果的论证称为“先验的”，而从结果到原因的论证称为“后验的”。包括客观（历史经验）先验和主观（主观估计）先验；后验概率是基于新的信息，修正原来的先验概率的概率估计。
  先验概率的计算比较简单，没有使用贝叶斯公式；而后验概率的计算，要使用贝叶斯公式，而且在利用样本资料计算逻辑概率时，还要使用理论概率分布，需要更多的数理统计知识。
  
  
